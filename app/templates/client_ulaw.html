<!DOCTYPE html>
<html>
<head>
    <title>µ-law Audio Client</title>
    <style>
        body { font-family: Arial, sans-serif; padding: 20px; }
        .controls { margin: 20px 0; }
        .recording { background: red; }
        .speaking { color: red; font-weight: bold; }
        .listening { color: green; font-weight: bold; }
        .playing { color: blue; font-weight: bold; }
        #volumeControl { margin: 10px 0; }
        #audioMeter {
            height: 20px;
            width: 200px;
            background: #eee;
            margin: 10px 0;
            border: 1px solid #999;
        }
        #audioLevel {
            height: 100%;
            width: 0%;
            background: #4CAF50;
            transition: width 100ms ease;
        }
    </style>
</head>
<body>
    <div class="controls">
        <button id="startBtn">Start Recording</button>
        <button id="stopBtn" disabled>Stop Recording</button>
        <div>
            <label for="volumeControl">Volume:</label>
            <input type="range" id="volumeControl" min="0" max="2" step="0.1" value="1">
        </div>
        <div id="audioMeter">
            <div id="audioLevel"></div>
        </div>
    </div>
    <div id="status">Ready</div>

    <script>
        let ws;
        let audioContext;
        let processor;
        let gainNode;
        let audioQueue = [];
        let isPlaying = false;
        let shouldPlayAudio = true;
        let currentSource = null;
        let analyser = null;

        const startBtn = document.getElementById('startBtn');
        const stopBtn = document.getElementById('stopBtn');
        const status = document.getElementById('status');
        const volumeControl = document.getElementById('volumeControl');
        const audioLevel = document.getElementById('audioLevel');
        const API_KEY = 'your-api-key-here';

        // µ-law conversion matching server format (8kHz, 8-bit)
        function linearToMuLaw(sample) {
            const MULAW_BIAS = 33;
            const MULAW_CLIP = 32635;

            sample = Math.min(Math.max(sample, -MULAW_CLIP), MULAW_CLIP);
            sample = Math.sign(sample) * Math.log(1 + MULAW_BIAS * Math.abs(sample)) / Math.log(1 + MULAW_BIAS);
            return Math.floor((sample + 1) * 127.5);
        }

        function muLawToLinear(ulaw) {
            const MULAW_BIAS = 33;
            const sign = (ulaw & 0x80) ? -1 : 1;
            ulaw = ~ulaw;
            return sign * (Math.exp((ulaw & 0x7F) / 127 * Math.log(1 + MULAW_BIAS)) - 1) / MULAW_BIAS * 32768;
        }

        async function init() {
            try {
                // Configure AudioContext for 8kHz
                audioContext = new (window.AudioContext || window.webkitAudioContext)({
                    sampleRate: 8000,
                    latencyHint: 'interactive'
                });

                gainNode = audioContext.createGain();
                gainNode.gain.value = volumeControl.value;
                gainNode.connect(audioContext.destination);

                // Setup WebSocket
                ws = new WebSocket('ws://localhost:5050/v1/voicebot/voicebot');
                setupWebSocketHandlers();
                setupVolumeControl();

            } catch (error) {
                console.error('Init error:', error);
                updateStatus('Error: ' + error.message);
            }
        }

        function setupWebSocketHandlers() {
            ws.addEventListener('open', () => {
                ws.send(JSON.stringify({
                    type: 'authentication',
                    'x-api-key': API_KEY
                }));
                updateStatus('Connected');
                startBtn.disabled = false;
            });

            ws.addEventListener('message', async (event) => {
                if (typeof event.data === 'string') {
                    handleTextMessage(event.data);
                } else if (event.data instanceof Blob) {
                    handleAudioMessage(event.data);
                }
            });

            ws.addEventListener('close', () => {
                updateStatus('Disconnected');
                startBtn.disabled = true;
                stopBtn.disabled = true;
            });

            ws.addEventListener('error', (error) => {
                console.error('WebSocket error:', error);
                updateStatus('Connection error');
            });
        }

        async function handleTextMessage(data) {
            try {
                const message = JSON.parse(data);
                if (message.event === 'user_started_speaking') {
                    shouldPlayAudio = false;
                    stopAudioPlayback();
                    clearAudioQueue();
                    updateStatus('User is speaking...', 'speaking');
                } else if (message.event === 'user_stopped_speaking') {
                    shouldPlayAudio = true;
                    updateStatus('Listening...', 'listening');
                    if (audioQueue.length > 0 && !isPlaying) {
                        playNextChunk();
                    }
                }
            } catch (e) {
                console.error('Message parse error:', e);
            }
        }

        async function handleAudioMessage(blob) {
            if (!shouldPlayAudio) return;

            const arrayBuffer = await blob.arrayBuffer();
            const audioData = new Uint8Array(arrayBuffer);

            // Convert µ-law to linear PCM
            const pcmData = new Float32Array(audioData.length);
            for (let i = 0; i < audioData.length; i++) {
                pcmData[i] = muLawToLinear(audioData[i]) / 32768.0;
            }

            audioQueue.push(pcmData);
            if (!isPlaying) playNextChunk();
        }

        function updateAudioLevel(level) {
            audioLevel.style.width = `${Math.min(100, level * 100)}%`;
        }

        function setupVolumeControl() {
            volumeControl.addEventListener('input', (e) => {
                if (gainNode) gainNode.gain.value = e.target.value;
            });
        }

        function updateStatus(message, className = '') {
            status.textContent = message;
            status.className = className ? `status ${className}` : 'status';
        }

        function stopAudioPlayback() {
            if (currentSource) {
                currentSource.stop();
                currentSource = null;
            }
            if (processor) {
                processor.disconnect();
                processor = null;
            }
            isPlaying = false;
        }

        function clearAudioQueue() {
            audioQueue = [];
        }

        async function playNextChunk() {
            if (!shouldPlayAudio || audioQueue.length === 0) {
                isPlaying = false;
                return;
            }

            isPlaying = true;
            const pcmData = audioQueue.shift();

            const buffer = audioContext.createBuffer(1, pcmData.length, 8000);
            buffer.copyToChannel(pcmData, 0);

            const source = audioContext.createBufferSource();
            source.buffer = buffer;
            source.connect(gainNode);
            currentSource = source;

            source.onended = () => {
                if (audioQueue.length > 0 && shouldPlayAudio) {
                    playNextChunk();
                } else {
                    isPlaying = false;
                    currentSource = null;
                    updateStatus(
                        shouldPlayAudio ? 'Listening...' : 'User is speaking...',
                        shouldPlayAudio ? 'listening' : 'speaking'
                    );
                }
            };

            source.start(0);
            updateStatus('Playing response...', 'playing');
        }

        startBtn.onclick = async () => {
            try {
                const stream = await navigator.mediaDevices.getUserMedia({
                    audio: {
                        channelCount: 1,
                        sampleRate: 8000,
                        echoCancellation: true,
                        noiseSuppression: true
                    }
                });

                const source = audioContext.createMediaStreamSource(stream);
                analyser = audioContext.createAnalyser();
                processor = audioContext.createScriptProcessor(2048, 1, 1);

                source.connect(analyser);
                analyser.connect(processor);
                processor.connect(audioContext.destination);

                const dataArray = new Uint8Array(analyser.frequencyBinCount);

                processor.onaudioprocess = (e) => {
                    if (ws.readyState === WebSocket.OPEN) {
                        const inputData = e.inputBuffer.getChannelData(0);
                        const mulawData = new Uint8Array(inputData.length);

                        analyser.getByteTimeDomainData(dataArray);
                        const level = Math.max(...Array.from(dataArray).map(x => Math.abs(x - 128) / 128));
                        updateAudioLevel(level);

                        // Convert to µ-law and send
                        for (let i = 0; i < inputData.length; i++) {
                            mulawData[i] = linearToMuLaw(inputData[i] * 32768);
                        }

                        ws.send(mulawData);
                    }
                };

                startBtn.disabled = true;
                stopBtn.disabled = false;
                startBtn.classList.add('recording');
                updateStatus('Recording...', 'recording');

            } catch (error) {
                console.error('Microphone error:', error);
                updateStatus('Microphone error: ' + error.message);
            }
        };

        stopBtn.onclick = () => {
            stopAudioPlayback();
            startBtn.disabled = false;
            stopBtn.disabled = true;
            startBtn.classList.remove('recording');
            updateStatus('Stopped recording');
            updateAudioLevel(0);
        };

        init();
    </script>
</body>
</html>
